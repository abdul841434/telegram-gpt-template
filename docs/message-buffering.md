# Буферизация сообщений

## Проблема

Когда пользователь отправляет несколько сообщений подряд быстрее, чем бот успевает ответить, возникает проблема множественных ответов:

```
Пользователь: привет!
Пользователь: как дела?
Пользователь: я сплю

Бот (без буферизации):
- Ответ на "привет!"
- Ответ на "как дела?"
- Ответ на "я сплю"
```

Это плохо, потому что:
- Пользователь получает 3 отдельных ответа вместо одного связного
- Тратится больше токенов LLM
- Контекст диалога фрагментируется

## Решение

Реализована система буферизации сообщений (`services/message_buffer.py`), которая:

1. **Накапливает быстрые сообщения** - если пользователь отправляет новое сообщение, пока обрабатывается предыдущее, оно добавляется в буфер
2. **Прерывает ожидание** - если в буфере появились новые сообщения, бот НЕ ждет ответа от текущего LLM-запроса
3. **Объединяет сообщения** - все накопленные сообщения склеиваются с переносом строки и отправляются одним запросом
4. **Непрерывный индикатор** - статус "печатает..." не прерывается на протяжении всего процесса

## Как это работает

```
0с:   Пользователь: "привет!"
      → Начали get_llm_response("привет!")
      → Запущен индикатор "печатает..."

2с:   Пользователь: "как дела?"
      → Добавлено в буфер
      → Прерываем ожидание первого ответа

2с:   Пользователь: "я сплю"
      → Добавлено в буфер

2с:   → Начали get_llm_response("привет!\nкак дела?\nя сплю")
      → Индикатор "печатает..." продолжает работать
      → Все три сообщения обрабатываются вместе

10с:  → Получили ответ от LLM
      → Сохранили в контекст
      → Отправили пользователю

Результат:
- 1 ответ на все 3 сообщения
- Пользователь ждал ~10 секунд вместо ~16
- Контекст содержит только то, что пользователь видел
```

## Архитектура

### MessageBuffer (`services/message_buffer.py`)

Класс для управления буферами сообщений каждого пользователя:

```python
message_buffer = MessageBuffer()

# В хандлере
should_process = await message_buffer.add_message(chat_id, text)
if not should_process:
    return  # Обработка уже идет

# ... обработка ...

while True:
    messages = await message_buffer.get_buffered_messages(chat_id)
    combined = "\n".join(messages)
    
    # Отправляем в LLM
    # Периодически проверяем has_buffered_messages()
    # Если True - прерываем ожидание
    
    has_more = await message_buffer.finish_processing(chat_id)
    if not has_more:
        break
```

### LLM Service рефакторинг

Функция `process_user_message` разделена на две:

1. **`get_llm_response(chat_id, message_text)`** 
   - Отправляет запрос к LLM
   - НЕ сохраняет в контекст
   - Возвращает `(llm_response, user_object)`

2. **`save_to_context_and_format(chat_id, user, user_message, llm_response)`**
   - Сохраняет диалог в базу
   - Форматирует для Telegram
   - Вызывается ТОЛЬКО если ответ будет показан пользователю

3. **`process_user_message(chat_id, message_text)`** (legacy)
   - Оставлена для обратной совместимости
   - Используется в напоминаниях и других местах
   - Вызывает внутри две функции выше

## Важные детали

### Почему не отменяем HTTP-запрос?

HTTP-запрос к OpenRouter API нельзя отменить после отправки. Даже если мы перестанем ждать ответ:
- Запрос обработается на сервере
- Токены будут потрачены
- Ответ придет, но мы его проигнорируем

Это нормально - мы экономим **время пользователя**, а токены - не критичны.

### Почему не сохраняем проигнорированный ответ?

Если сохранить в контекст ответ, который пользователь не увидел:
- Контекст будет содержать "невидимые" для пользователя сообщения
- Это запутает LLM в следующих ответах
- Пользователь может спросить "что ты имел в виду?", а бота речь идет о невидимом сообщении

### Период проверки буфера

В цикле ожидания ответа проверка происходит каждые 100мс (`await asyncio.sleep(0.1)`):

```python
while not llm_task.done():
    await asyncio.sleep(0.1)
    
    if await message_buffer.has_buffered_messages(chat_id):
        # Прерываем ожидание
        break
```

Это достаточно быстро для реактивности и достаточно редко, чтобы не нагружать систему.

## Ограничения и будущие улучшения

### Текущие ограничения

1. **Буфер в памяти** - при перезапуске бота состояние теряется
2. **Только текстовые сообщения** - фото и видео пока используют старый подход
3. **Нет персистентности** - буфер существует только в рантайме

### Возможные улучшения

1. **Redis для буфера** - сохранять состояние между перезапусками
2. **Буферизация медиа** - применить тот же подход к фото/видео
3. **Умная склейка** - добавлять метки времени между сообщениями
4. **Настройка периода проверки** - конфигурируемый параметр

## Тестирование

Для тестирования отправьте несколько сообщений подряд:

```
привет
как дела
что нового
расскажи о себе
```

Ожидаемый результат:
- Индикатор "печатает..." появился и не исчез
- Бот ответил **один раз** на все сообщения
- В логах видно объединение сообщений:
  ```
  USER123TOLLM (объединено 4 сообщений): привет\nкак дела\nчто нового\nрасскажи о себе
  ```

## Файлы

- `services/message_buffer.py` - реализация буфера
- `services/llm_service.py` - рефакторинг функций LLM
- `handlers/message_handlers.py` - обновленная логика хандлеров
- `docs/message-buffering.md` - эта документация

